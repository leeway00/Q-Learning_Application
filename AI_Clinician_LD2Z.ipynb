{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AI Clinician with LD2Z Scheduler\n",
    "\n",
    "This notebook demonstrates the implementation of Q-learning for medical decision-making (AI Clinician approach) using the LD2Z learning rate scheduler.\n",
    "\n",
    "## Background\n",
    "\n",
    "- **AI Clinician**: A reinforcement learning approach for optimal treatment strategies in intensive care (sepsis management)\n",
    "- **LD2Z Scheduler**: Learning rate schedule proportional to 1/t^(2/3), which provides optimal convergence properties for Q-learning\n",
    "- **Goal**: Analyze the statistical properties and convergence behavior of Q-learning with LD2Z scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from q_learning_agent import QLearningAgent\n",
    "from environment import SimpleMedicalMDP\n",
    "from statistical_analysis import StatisticalAnalyzer\n",
    "from ld2z_scheduler import LD2ZScheduler\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Configure plotting\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. LD2Z Scheduler Analysis\n",
    "\n",
    "First, let's visualize the LD2Z learning rate schedule and compare it with other common schedules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create schedulers\n",
    "ld2z = LD2ZScheduler(initial_lr=1.0, exponent=2/3)\n",
    "constant = LD2ZScheduler(initial_lr=0.1, exponent=0)  # Constant\n",
    "standard = LD2ZScheduler(initial_lr=1.0, exponent=1.0)  # 1/t schedule\n",
    "\n",
    "# Generate learning rates\n",
    "steps = np.arange(1, 1001)\n",
    "lr_ld2z = [ld2z.get_learning_rate(t) for t in steps]\n",
    "lr_constant = [0.1 for t in steps]\n",
    "lr_standard = [standard.get_learning_rate(t) for t in steps]\n",
    "\n",
    "# Plot\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Linear scale\n",
    "axes[0].plot(steps, lr_ld2z, label='LD2Z (1/t^(2/3))', linewidth=2)\n",
    "axes[0].plot(steps, lr_standard, label='Standard (1/t)', linewidth=2)\n",
    "axes[0].plot(steps, lr_constant, label='Constant (0.1)', linewidth=2)\n",
    "axes[0].set_xlabel('Step')\n",
    "axes[0].set_ylabel('Learning Rate')\n",
    "axes[0].set_title('Learning Rate Schedules (Linear Scale)')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Log scale\n",
    "axes[1].plot(steps, lr_ld2z, label='LD2Z (1/t^(2/3))', linewidth=2)\n",
    "axes[1].plot(steps, lr_standard, label='Standard (1/t)', linewidth=2)\n",
    "axes[1].plot(steps, lr_constant, label='Constant (0.1)', linewidth=2)\n",
    "axes[1].set_xlabel('Step')\n",
    "axes[1].set_ylabel('Learning Rate')\n",
    "axes[1].set_title('Learning Rate Schedules (Log Scale)')\n",
    "axes[1].set_xscale('log')\n",
    "axes[1].set_yscale('log')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"LD2Z Schedule Properties:\")\n",
    "print(f\"  Initial learning rate (t=1): {lr_ld2z[0]:.4f}\")\n",
    "print(f\"  Learning rate at t=100: {lr_ld2z[99]:.4f}\")\n",
    "print(f\"  Learning rate at t=1000: {lr_ld2z[999]:.4f}\")\n",
    "print(f\"\\nDecay is slower than 1/t but faster than constant\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Create Medical Decision-Making Environment\n",
    "\n",
    "We create a simplified MDP that simulates medical interventions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create environment\n",
    "n_states = 50\n",
    "n_actions = 5\n",
    "env = SimpleMedicalMDP(n_states=n_states, n_actions=n_actions, seed=42)\n",
    "\n",
    "print(f\"Environment created with {n_states} states and {n_actions} actions\")\n",
    "print(f\"Number of terminal states: {len(env.terminal_states)}\")\n",
    "\n",
    "# Visualize reward structure\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Heatmap of rewards\n",
    "im1 = axes[0].imshow(env.R, aspect='auto', cmap='RdYlGn')\n",
    "axes[0].set_xlabel('Action')\n",
    "axes[0].set_ylabel('State')\n",
    "axes[0].set_title('Reward Structure R(s, a)')\n",
    "plt.colorbar(im1, ax=axes[0])\n",
    "\n",
    "# Distribution of rewards\n",
    "axes[1].hist(env.R.flatten(), bins=30, alpha=0.7, edgecolor='black')\n",
    "axes[1].set_xlabel('Reward')\n",
    "axes[1].set_ylabel('Frequency')\n",
    "axes[1].set_title('Distribution of Immediate Rewards')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Train Q-Learning Agent with LD2Z Scheduler\n",
    "\n",
    "Now we train the agent using the LD2Z scheduler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize agent with LD2Z scheduler\n",
    "agent_ld2z = QLearningAgent(\n",
    "    n_states=n_states,\n",
    "    n_actions=n_actions,\n",
    "    gamma=0.99,\n",
    "    initial_lr=1.0,\n",
    "    use_ld2z=True\n",
    ")\n",
    "\n",
    "# Initialize analyzer\n",
    "analyzer_ld2z = StatisticalAnalyzer()\n",
    "\n",
    "# Training parameters\n",
    "n_episodes = 1000\n",
    "max_steps_per_episode = 100\n",
    "epsilon_start = 1.0\n",
    "epsilon_end = 0.01\n",
    "epsilon_decay = 0.995\n",
    "\n",
    "print(\"Training Q-Learning Agent with LD2Z Scheduler...\")\n",
    "print(f\"Episodes: {n_episodes}, Max Steps: {max_steps_per_episode}\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "epsilon = epsilon_start\n",
    "episode_rewards_ld2z = []\n",
    "\n",
    "for episode in range(n_episodes):\n",
    "    state = env.reset()\n",
    "    episode_reward = 0\n",
    "    \n",
    "    for step in range(max_steps_per_episode):\n",
    "        # Select and take action\n",
    "        action = agent_ld2z.select_action(state, epsilon)\n",
    "        next_state, reward, done, info = env.step(action)\n",
    "        \n",
    "        # Update Q-values\n",
    "        td_error = agent_ld2z.update(state, action, reward, next_state, done)\n",
    "        \n",
    "        # Record statistics\n",
    "        lr = agent_ld2z.scheduler.get_learning_rate(\n",
    "            agent_ld2z.visit_counts[state, action]\n",
    "        )\n",
    "        analyzer_ld2z.record_step(agent_ld2z.Q, td_error, reward, lr)\n",
    "        \n",
    "        episode_reward += reward\n",
    "        state = next_state\n",
    "        \n",
    "        if done:\n",
    "            break\n",
    "    \n",
    "    episode_rewards_ld2z.append(episode_reward)\n",
    "    epsilon = max(epsilon_end, epsilon * epsilon_decay)\n",
    "    \n",
    "    if (episode + 1) % 100 == 0:\n",
    "        avg_reward = np.mean(episode_rewards_ld2z[-100:])\n",
    "        print(f\"Episode {episode + 1}: Avg Reward = {avg_reward:.2f}, Epsilon = {epsilon:.3f}\")\n",
    "\n",
    "print(\"-\" * 60)\n",
    "print(\"Training completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Train Agent with Constant Learning Rate (for comparison)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset environment\n",
    "env = SimpleMedicalMDP(n_states=n_states, n_actions=n_actions, seed=42)\n",
    "\n",
    "# Initialize agent with constant learning rate\n",
    "agent_const = QLearningAgent(\n",
    "    n_states=n_states,\n",
    "    n_actions=n_actions,\n",
    "    gamma=0.99,\n",
    "    initial_lr=0.1,\n",
    "    use_ld2z=False\n",
    ")\n",
    "\n",
    "analyzer_const = StatisticalAnalyzer()\n",
    "\n",
    "print(\"Training Q-Learning Agent with Constant Learning Rate...\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "epsilon = epsilon_start\n",
    "episode_rewards_const = []\n",
    "\n",
    "for episode in range(n_episodes):\n",
    "    state = env.reset()\n",
    "    episode_reward = 0\n",
    "    \n",
    "    for step in range(max_steps_per_episode):\n",
    "        action = agent_const.select_action(state, epsilon)\n",
    "        next_state, reward, done, info = env.step(action)\n",
    "        td_error = agent_const.update(state, action, reward, next_state, done)\n",
    "        \n",
    "        analyzer_const.record_step(agent_const.Q, td_error, reward, agent_const.constant_lr)\n",
    "        \n",
    "        episode_reward += reward\n",
    "        state = next_state\n",
    "        \n",
    "        if done:\n",
    "            break\n",
    "    \n",
    "    episode_rewards_const.append(episode_reward)\n",
    "    epsilon = max(epsilon_end, epsilon * epsilon_decay)\n",
    "    \n",
    "    if (episode + 1) % 100 == 0:\n",
    "        avg_reward = np.mean(episode_rewards_const[-100:])\n",
    "        print(f\"Episode {episode + 1}: Avg Reward = {avg_reward:.2f}\")\n",
    "\n",
    "print(\"-\" * 60)\n",
    "print(\"Training completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Statistical Analysis and Comparison\n",
    "\n",
    "Now we analyze and compare the statistical properties of both approaches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate reports\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"STATISTICAL ANALYSIS RESULTS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(\"\\n1. LD2Z Scheduler:\")\n",
    "print(analyzer_ld2z.generate_report())\n",
    "\n",
    "print(\"\\n2. Constant Learning Rate:\")\n",
    "print(analyzer_const.generate_report())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Convergence Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot convergence comparison\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# TD Error comparison\n",
    "axes[0, 0].plot(analyzer_ld2z.history['td_errors'], alpha=0.6, label='LD2Z')\n",
    "axes[0, 0].plot(analyzer_const.history['td_errors'], alpha=0.6, label='Constant LR')\n",
    "axes[0, 0].set_xlabel('Step')\n",
    "axes[0, 0].set_ylabel('TD Error')\n",
    "axes[0, 0].set_title('TD Error Convergence')\n",
    "axes[0, 0].set_yscale('log')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Episode rewards comparison\n",
    "window = 50\n",
    "rewards_ld2z_ma = np.convolve(episode_rewards_ld2z, np.ones(window)/window, mode='valid')\n",
    "rewards_const_ma = np.convolve(episode_rewards_const, np.ones(window)/window, mode='valid')\n",
    "\n",
    "axes[0, 1].plot(rewards_ld2z_ma, label='LD2Z', linewidth=2)\n",
    "axes[0, 1].plot(rewards_const_ma, label='Constant LR', linewidth=2)\n",
    "axes[0, 1].set_xlabel('Episode')\n",
    "axes[0, 1].set_ylabel('Reward (Moving Average)')\n",
    "axes[0, 1].set_title('Training Rewards')\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Value function changes\n",
    "axes[1, 0].plot(analyzer_ld2z.history['value_differences'], alpha=0.6, label='LD2Z')\n",
    "axes[1, 0].plot(analyzer_const.history['value_differences'], alpha=0.6, label='Constant LR')\n",
    "axes[1, 0].set_xlabel('Step')\n",
    "axes[1, 0].set_ylabel('Value Function Change')\n",
    "axes[1, 0].set_title('Value Function Convergence')\n",
    "axes[1, 0].set_yscale('log')\n",
    "axes[1, 0].legend()\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Q-value distributions\n",
    "axes[1, 1].hist(agent_ld2z.Q.flatten(), bins=30, alpha=0.6, label='LD2Z', edgecolor='black')\n",
    "axes[1, 1].hist(agent_const.Q.flatten(), bins=30, alpha=0.6, label='Constant LR', edgecolor='black')\n",
    "axes[1, 1].set_xlabel('Q-value')\n",
    "axes[1, 1].set_ylabel('Frequency')\n",
    "axes[1, 1].set_title('Final Q-value Distribution')\n",
    "axes[1, 1].legend()\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Detailed Convergence Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LD2Z detailed convergence\n",
    "print(\"LD2Z Scheduler - Detailed Convergence Analysis:\")\n",
    "fig_ld2z = analyzer_ld2z.plot_convergence()\n",
    "plt.show()\n",
    "\n",
    "# Constant LR detailed convergence\n",
    "print(\"\\nConstant Learning Rate - Detailed Convergence Analysis:\")\n",
    "fig_const = analyzer_const.plot_convergence()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Policy Analysis\n",
    "\n",
    "Compare the learned policies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get learned policies\n",
    "policy_ld2z = agent_ld2z.get_policy()\n",
    "policy_const = agent_const.get_policy()\n",
    "\n",
    "# Compute value functions\n",
    "values_ld2z = agent_ld2z.get_value_function()\n",
    "values_const = agent_const.get_value_function()\n",
    "\n",
    "# Visualize policies\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# LD2Z policy\n",
    "axes[0, 0].bar(range(n_states), policy_ld2z, alpha=0.7)\n",
    "axes[0, 0].set_xlabel('State')\n",
    "axes[0, 0].set_ylabel('Action')\n",
    "axes[0, 0].set_title('LD2Z Learned Policy')\n",
    "axes[0, 0].set_ylim([-0.5, n_actions - 0.5])\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Constant LR policy\n",
    "axes[0, 1].bar(range(n_states), policy_const, alpha=0.7, color='orange')\n",
    "axes[0, 1].set_xlabel('State')\n",
    "axes[0, 1].set_ylabel('Action')\n",
    "axes[0, 1].set_title('Constant LR Learned Policy')\n",
    "axes[0, 1].set_ylim([-0.5, n_actions - 0.5])\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# LD2Z value function\n",
    "axes[1, 0].plot(values_ld2z, marker='o', alpha=0.7)\n",
    "axes[1, 0].set_xlabel('State')\n",
    "axes[1, 0].set_ylabel('Value')\n",
    "axes[1, 0].set_title('LD2Z Value Function')\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Constant LR value function\n",
    "axes[1, 1].plot(values_const, marker='o', alpha=0.7, color='orange')\n",
    "axes[1, 1].set_xlabel('State')\n",
    "axes[1, 1].set_ylabel('Value')\n",
    "axes[1, 1].set_title('Constant LR Value Function')\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Policy agreement\n",
    "policy_agreement = np.mean(policy_ld2z == policy_const)\n",
    "print(f\"\\nPolicy Agreement: {policy_agreement * 100:.2f}%\")\n",
    "print(f\"Mean value (LD2Z): {np.mean(values_ld2z):.4f}\")\n",
    "print(f\"Mean value (Constant): {np.mean(values_const):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Statistical Properties Summary\n",
    "\n",
    "Key findings from the analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_ld2z = analyzer_ld2z.compute_convergence_metrics()\n",
    "metrics_const = analyzer_const.compute_convergence_metrics()\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"SUMMARY OF STATISTICAL PROPERTIES\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(\"\\n1. Convergence Speed:\")\n",
    "if 'convergence_ratio' in metrics_ld2z:\n",
    "    print(f\"   LD2Z: {metrics_ld2z['convergence_ratio']:.4f}\")\n",
    "if 'convergence_ratio' in metrics_const:\n",
    "    print(f\"   Constant: {metrics_const['convergence_ratio']:.4f}\")\n",
    "print(\"   (Lower is better - indicates faster convergence)\")\n",
    "\n",
    "print(\"\\n2. Final TD Error:\")\n",
    "print(f\"   LD2Z: {metrics_ld2z.get('mean_recent_td_error', 0):.6f}\")\n",
    "print(f\"   Constant: {metrics_const.get('mean_recent_td_error', 0):.6f}\")\n",
    "print(\"   (Lower is better)\")\n",
    "\n",
    "print(\"\\n3. Reward Performance:\")\n",
    "print(f\"   LD2Z: {metrics_ld2z.get('mean_reward', 0):.4f} ± {metrics_ld2z.get('std_reward', 0):.4f}\")\n",
    "print(f\"   Constant: {metrics_const.get('mean_reward', 0):.4f} ± {metrics_const.get('std_reward', 0):.4f}\")\n",
    "\n",
    "print(\"\\n4. Value Function Stability:\")\n",
    "print(f\"   LD2Z: {metrics_ld2z.get('mean_value_change', 0):.6f}\")\n",
    "print(f\"   Constant: {metrics_const.get('mean_value_change', 0):.6f}\")\n",
    "print(\"   (Lower indicates more stable convergence)\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"\\nKey Insights:\")\n",
    "print(\"- LD2Z scheduler provides adaptive learning rates\")\n",
    "print(\"- Learning rate decays as 1/t^(2/3), balancing exploration and convergence\")\n",
    "print(\"- Theoretical guarantees for optimal convergence in Q-learning\")\n",
    "print(\"- Particularly effective for medical decision-making with sparse rewards\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Conclusion\n",
    "\n",
    "This notebook demonstrated:\n",
    "1. Implementation of Q-learning with LD2Z scheduler for AI Clinician application\n",
    "2. Comparison with constant learning rate baseline\n",
    "3. Statistical analysis of convergence properties\n",
    "4. Visualization of learning dynamics and policy quality\n",
    "\n",
    "The LD2Z scheduler shows promise for medical reinforcement learning applications where:\n",
    "- Optimal convergence guarantees are important\n",
    "- State-action spaces can be large\n",
    "- Sample efficiency is critical"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
